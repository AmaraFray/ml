{'KNN': {'imp': 'import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.datasets import load_breast_cancer', 'data': "dataset=load_breast_cancer()\nX_train,X_test,Y_train,Y_test = train_test_split(dataset['data'],dataset['target'], random_state=1984)", 'model': '\ntraining_accuracy=[]\ntest_accuracy=[]\nK=range(1,15)\nfor number_of_neighbors in K:\n  KNN=KNeighborsClassifier(n_neighbors=number_of_neighbors)\n  KNN.fit(X_train,Y_train)\n  training_accuracy.append(KNN.score(X_train,Y_train))\n  test_accuracy.append(KNN.score(X_test,Y_test))\n  pred = KNN.predict(X_test)\n  acc = accuracy_score(Y_test, pred)\n  print("K value = {}\t Accuracy = {}".format(number_of_neighbors,acc))\n                  ', 'plot': 'plt.plot(K,training_accuracy, label="Accuracy in the training dataset")\nplt.plot(K,test_accuracy, label="Accuracy in the testing dataset")\nplt.ylabel("Accuracy")\nplt.xlabel("K value")\nplt.legend()', 'all': 'import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.datasets import load_breast_cancer\ndataset=load_breast_cancer()\nX_train,X_test,Y_train,Y_test = train_test_split(dataset[\'data\'],dataset[\'target\'], random_state=1984)\n\ntraining_accuracy=[]\ntest_accuracy=[]\nK=range(1,15)\nfor number_of_neighbors in K:\n  KNN=KNeighborsClassifier(n_neighbors=number_of_neighbors)\n  KNN.fit(X_train,Y_train)\n  training_accuracy.append(KNN.score(X_train,Y_train))\n  test_accuracy.append(KNN.score(X_test,Y_test))\n  pred = KNN.predict(X_test)\n  acc = accuracy_score(Y_test, pred)\n  print("K value = {}\t Accuracy = {}".format(number_of_neighbors,acc))\n                  \nplt.plot(K,training_accuracy, label="Accuracy in the training dataset")\nplt.plot(K,test_accuracy, label="Accuracy in the testing dataset")\nplt.ylabel("Accuracy")\nplt.xlabel("K value")\nplt.legend()'}, 'DT': {'imp': '\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.datasets import load_iris\nfrom sklearn import metrics\nfrom sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier\n\nimport matplotlib.pyplot as plt\n        ', 'data': "\ndata = load_iris()\n\ndf = pd.DataFrame(data.data, columns=data.feature_names)\nX = df.copy()\ndf['target'] = data.target\nY = df['target']\n        ", 'model': "\ndtree = DecisionTreeClassifier()\n# id3_tree = DecisionTreeClassifier(criterion = 'entropy')\n# cart_tree = DecisionTreeClassifier(criterion = 'gini')\nfitTree = dtree.fit(X, Y)\ntree.plot_tree(fitTree, feature_names=list(df.columns))\ny_pred = dtree.predict(X)\n        ", 'plot': '\nconfusion_matrix = metrics.confusion_matrix(Y, y_pred)\ncm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = np.unique(y_pred))\n\nplt.figure(figsize=(15,15))\ncm_display.plot()\n\nplt.show()\n        ', 'all': "\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.datasets import load_iris\nfrom sklearn import metrics\nfrom sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier\n\nimport matplotlib.pyplot as plt\n        \n\ndata = load_iris()\n\ndf = pd.DataFrame(data.data, columns=data.feature_names)\nX = df.copy()\ndf['target'] = data.target\nY = df['target']\n        \n\ndtree = DecisionTreeClassifier()\n# id3_tree = DecisionTreeClassifier(criterion = 'entropy')\n# cart_tree = DecisionTreeClassifier(criterion = 'gini')\nfitTree = dtree.fit(X, Y)\ntree.plot_tree(fitTree, feature_names=list(df.columns))\ny_pred = dtree.predict(X)\n        \n\nconfusion_matrix = metrics.confusion_matrix(Y, y_pred)\ncm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = np.unique(y_pred))\n\nplt.figure(figsize=(15,15))\ncm_display.plot()\n\nplt.show()\n        "}, 'SVM': {'imp': '\nfrom sklearn.svm import SVC\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import GridSearchCV\n        ', 'data': '\nX = np.random.randn(20, 2)\ny = [0 if x[0] + x[1] < 0 else 1 for x in X]\n\nplt.scatter(X[:, 0], X[:, 1], c=y)\nplt.show()\n\nsvc = SVC(kernel="linear")\nfittedsvc = svc.fit(X, y)\n        ', 'model': '\nsvc_small_c = SVC(C=0.1)\nsvc_small_c.fit(X, y)\nparam_grid = {\'C\': [0.001, 0.01, 0.1, 1, 5, 10, 100]}\ngrid_search = GridSearchCV(SVC(), param_grid, cv=5)\ngrid_search.fit(X, y)\n\nprint("Cross-Validation Errors:", grid_search.cv_results_[\'mean_test_score\'])\nprint("Best Value of C:", grid_search.best_params_[\'C\'])\n\nX_test = np.random.randn(20, 2)\n\ny_pred = grid_search.best_estimator_.predict(X_test)\naccuracy = sum(y_pred == [0 if x[0] + x[1] < 0 else 1 for x in X_test]) / len(X_test)\nprint(f"Accuracy on Test Data: {accuracy*100:.2f}%")\n\nprint("Number of Support Vectors:", len(grid_search.best_estimator_.support_vectors_))\n        ', 'plot': "\ndef plot_svc_decision_function(model, ax=None, plot_support=True):\n    if ax is None:\n        ax = plt.gca()\n    xlim = ax.get_xlim()\n    ylim = ax.get_ylim()\n\n    # create grid to evaluate model\n    x = np.linspace(xlim[0], xlim[1], 30)\n    y = np.linspace(ylim[0], ylim[1], 30)\n    Y, X = np.meshgrid(y, x)\n    xy = np.vstack([X.ravel(), Y.ravel()]).T\n    P = model.decision_function(xy).reshape(X.shape)\n\n    # plot decision boundary and margins\n    ax.contour(X, Y, P, colors='k',\n               levels=[-1, 0, 1], alpha=0.5,\n               linestyles=['--', '-', '--'])\n\n    # plot support vectors\n    if plot_support:\n        ax.scatter(model.support_vectors_[:, 0],\n                   model.support_vectors_[:, 1],\n                   s=300, linewidth=1, facecolors='none');\n    ax.set_xlim(xlim)\n    ax.set_ylim(ylim)\n\nplt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\nplot_svc_decision_function(svc)\nplt.show()\n        ", 'all': '\nfrom sklearn.svm import SVC\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import GridSearchCV\n        \n\nX = np.random.randn(20, 2)\ny = [0 if x[0] + x[1] < 0 else 1 for x in X]\n\nplt.scatter(X[:, 0], X[:, 1], c=y)\nplt.show()\n\nsvc = SVC(kernel="linear")\nfittedsvc = svc.fit(X, y)\n        \n\nsvc_small_c = SVC(C=0.1)\nsvc_small_c.fit(X, y)\nparam_grid = {\'C\': [0.001, 0.01, 0.1, 1, 5, 10, 100]}\ngrid_search = GridSearchCV(SVC(), param_grid, cv=5)\ngrid_search.fit(X, y)\n\nprint("Cross-Validation Errors:", grid_search.cv_results_[\'mean_test_score\'])\nprint("Best Value of C:", grid_search.best_params_[\'C\'])\n\nX_test = np.random.randn(20, 2)\n\ny_pred = grid_search.best_estimator_.predict(X_test)\naccuracy = sum(y_pred == [0 if x[0] + x[1] < 0 else 1 for x in X_test]) / len(X_test)\nprint(f"Accuracy on Test Data: {accuracy*100:.2f}%")\n\nprint("Number of Support Vectors:", len(grid_search.best_estimator_.support_vectors_))\n        \n\ndef plot_svc_decision_function(model, ax=None, plot_support=True):\n    if ax is None:\n        ax = plt.gca()\n    xlim = ax.get_xlim()\n    ylim = ax.get_ylim()\n\n    # create grid to evaluate model\n    x = np.linspace(xlim[0], xlim[1], 30)\n    y = np.linspace(ylim[0], ylim[1], 30)\n    Y, X = np.meshgrid(y, x)\n    xy = np.vstack([X.ravel(), Y.ravel()]).T\n    P = model.decision_function(xy).reshape(X.shape)\n\n    # plot decision boundary and margins\n    ax.contour(X, Y, P, colors=\'k\',\n               levels=[-1, 0, 1], alpha=0.5,\n               linestyles=[\'--\', \'-\', \'--\'])\n\n    # plot support vectors\n    if plot_support:\n        ax.scatter(model.support_vectors_[:, 0],\n                   model.support_vectors_[:, 1],\n                   s=300, linewidth=1, facecolors=\'none\');\n    ax.set_xlim(xlim)\n    ax.set_ylim(ylim)\n\nplt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=\'autumn\')\nplot_svc_decision_function(svc)\nplt.show()\n        '}, 'MLR': {'imp': '\nfrom sklearn.linear_model import LinearRegression\nimport pandas as pd\nimport matplotlib.pyplot as plt\n        ', 'data': "\nX = df[['age', 'cls_did_eval', 'cls_students', 'cls_perc_eval',\n       'bty_f1lower', 'bty_f1upper', 'bty_f2upper', 'bty_m1lower',\n       'bty_m1upper', 'bty_m2upper', 'bty_avg']]\ny = df.score.values\n        ", 'model': '\nmodel = LinearRegression()\nmodel.fit(X, y)\nmodel.score(X, y)\nprint(f"The slope and intercept are {model.coef_[0]},  {model.intercept_}")\n        ', 'extra': "\nimport sklearn.metrics as metrics\nplt.scatter(df.bty_avg, df.score, color='red')\nmodel.score(X, y)\ndf.corr()['bty_avg']['bty_f1upper']\nexplained_variance=metrics.explained_variance_score(y_true, y_pred)\nmean_absolute_error=metrics.mean_absolute_error(y_true, y_pred) \nmse=metrics.mean_squared_error(y_true, y_pred) \nmean_squared_log_error=metrics.mean_squared_log_error(y_true, y_pred)\nmedian_absolute_error=metrics.median_absolute_error(y_true, y_pred)\nr2=metrics.r2_score(y_true, y_pred)\n        ", 'all': '\nfrom sklearn.linear_model import LinearRegression\nimport pandas as pd\nimport matplotlib.pyplot as plt\n        \n\nX = df[[\'age\', \'cls_did_eval\', \'cls_students\', \'cls_perc_eval\',\n       \'bty_f1lower\', \'bty_f1upper\', \'bty_f2upper\', \'bty_m1lower\',\n       \'bty_m1upper\', \'bty_m2upper\', \'bty_avg\']]\ny = df.score.values\n        \n\nmodel = LinearRegression()\nmodel.fit(X, y)\nmodel.score(X, y)\nprint(f"The slope and intercept are {model.coef_[0]},  {model.intercept_}")\n        \n\nimport sklearn.metrics as metrics\nplt.scatter(df.bty_avg, df.score, color=\'red\')\nmodel.score(X, y)\ndf.corr()[\'bty_avg\'][\'bty_f1upper\']\nexplained_variance=metrics.explained_variance_score(y_true, y_pred)\nmean_absolute_error=metrics.mean_absolute_error(y_true, y_pred) \nmse=metrics.mean_squared_error(y_true, y_pred) \nmean_squared_log_error=metrics.mean_squared_log_error(y_true, y_pred)\nmedian_absolute_error=metrics.median_absolute_error(y_true, y_pred)\nr2=metrics.r2_score(y_true, y_pred)\n        '}}
